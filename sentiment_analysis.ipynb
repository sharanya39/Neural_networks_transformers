{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-World Use Case: Movie Review Sentiment Analysis:\n",
    "\n",
    "    We'll simulate a scenario where our model predicts the sentiment of movie reviews. This approach is commonly used in the movie industry for analyzing audience feedback. Our final model will be able to infer whether a given movie review is positive or negative, helping movie studios gauge the reception of their films in real-time.\n",
    "\n",
    "    1.Understanding Neural Networks through Movie Reviews\n",
    "        We will develop a neural network to analyze movie reviews and predict if the sentiment is positive or negative. This model can be used by film studios or streaming platforms to automate sentiment analysis.\n",
    "    Step 2: Implementing Sentiment Analysis Using PyTorch\n",
    "\n",
    "    import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Sample movie review data (simplified example)\n",
    "reviews = [\"I loved this movie!\", \"It was terrible.\", \"Absolutely fantastic!\", \"Not worth watching.\"] \n",
    "labels = [1, 0, 1, 0] # 1 = positive, 0 = negative\n",
    "# Convert reviews into numerical data (simple encoding)\n",
    "vocab = {word: i for i, word in enumerate(set(\" \".join(reviews).split()))}\n",
    "encoded_reviews = [[vocab[word] for word in review.split()] for review in reviews]\n",
    "# Simple PyTorch model\n",
    "class SentimentNN(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(SentimentNN, self).__init__() \n",
    "        self.fc = nn.Linear(vocab_size, 1) \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return self.sigmoid(x)\n",
    "# Dummy training loop for epoch in range(10):\n",
    "for i, review in enumerate(encoded_reviews):\n",
    "    input_data = torch.tensor(review, dtype=torch.float32)  \n",
    "    label = torch.tensor([labels[i]], dtype=torch.float32)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_data)\n",
    "    loss = criterion(output, label) \n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "    print(\"Training complete!\")\n",
    "#Step 3: Inference - Using the Model for Predictions\n",
    "def predict_sentiment(review):\n",
    "    # Convert review into numerical format\n",
    "    review_words = review.split()\n",
    "    encoded_review = [vocab.get(word, 0) for word in review_words] # Default to 0 if word is not in vocab input_data = torch.tensor(encoded_review, dtype=torch.float32)\n",
    "    # Make prediction\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_data)\n",
    "        sentiment = \"Positive\" if prediction.item() >= 0.5 else \"Negative\"\n",
    "        return sentiment\n",
    "# Example usage\n",
    "new_review = \"This movie was amazing and thrilling!\"\n",
    "print(f\"Review: {new_review}\")\n",
    "print(f\"Predicted Sentiment: {predict_sentiment(new_review)}\")\n",
    "\n",
    "#Step 5: Building a Simplified Transformer Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "# Movie review vocabulary size (dummy data)\n",
    "vocab_size = 100\n",
    "embed_size = 64\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "class SimpleTransformer(nn.Module):\n",
    "     def __init__(self, vocab_size, embed_size, num_heads, num_layers):\n",
    "        super(SimpleTransformer, self).__init__() \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size) \n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_size, nhead=num_heads, num_encoder_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "     def forward(self, src, tgt):\n",
    "        src = self.embedding(src)\n",
    "        tgt = self.embedding(tgt)\n",
    "        return self.fc(self.transformer(src, tgt))\n",
    "# Example training data (encoded as numbers)\n",
    "src = torch.randint(0, vocab_size, (10, 32)) #10 sequences of length 32\n",
    "tgt = torch.randint(0, vocab_size, (10, 32))\n",
    "# Model, loss, and optimizer\n",
    "model = SimpleTransformer(vocab_size, embed_size, num_heads, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "for epoch in range(5): # Simplified training loop\n",
    "    optimizer.zero_grad()\n",
    "    output = model(src, tgt)\n",
    "    loss = criterion(output.view(-1, vocab_size), tgt.view(-1)) \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(\"Transformer model training complete!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Implementing Sentiment Analysis Using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m input_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(review, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)  \n\u001b[1;32m     22\u001b[0m label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([labels[i]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 23\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     24\u001b[0m output \u001b[38;5;241m=\u001b[39m model(input_data)\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, label) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Sample movie review data (simplified example)\n",
    "reviews = [\"I loved this movie!\", \"It was terrible.\", \"Absolutely fantastic!\", \"Not worth watching.\"] \n",
    "labels = [1, 0, 1, 0] # 1 = positive, 0 = negative\n",
    "# Convert reviews into numerical data (simple encoding)\n",
    "vocab = {word: i for i, word in enumerate(set(\" \".join(reviews).split()))}\n",
    "encoded_reviews = [[vocab[word] for word in review.split()] for review in reviews]\n",
    "# Simple PyTorch model\n",
    "class SentimentNN(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(SentimentNN, self).__init__() \n",
    "        self.fc = nn.Linear(vocab_size, 1) \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return self.sigmoid(x)\n",
    "# Dummy training loop for epoch in range(10):\n",
    "for i, review in enumerate(encoded_reviews):\n",
    "    input_data = torch.tensor(review, dtype=torch.float32)  \n",
    "    label = torch.tensor([labels[i]], dtype=torch.float32)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_data)\n",
    "    loss = criterion(output, label) \n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "    print(\"Training complete!\")\n",
    "#Step 3: Inference - Using the Model for Predictions\n",
    "def predict_sentiment(review):\n",
    "    # Convert review into numerical format\n",
    "    review_words = review.split()\n",
    "    encoded_review = [vocab.get(word, 0) for word in review_words] # Default to 0 if word is not in vocab input_data = torch.tensor(encoded_review, dtype=torch.float32)\n",
    "    # Make prediction\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_data)\n",
    "        sentiment = \"Positive\" if prediction.item() >= 0.5 else \"Negative\"\n",
    "        return sentiment\n",
    "# Example usage\n",
    "new_review = \"This movie was amazing and thrilling!\"\n",
    "print(f\"Review: {new_review}\")\n",
    "print(f\"Predicted Sentiment: {predict_sentiment(new_review)}\")\n",
    "\n",
    "#Step 5: Building a Simplified Transformer Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "# Movie review vocabulary size (dummy data)\n",
    "vocab_size = 100\n",
    "embed_size = 64\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "class SimpleTransformer(nn.Module):\n",
    "     def __init__(self, vocab_size, embed_size, num_heads, num_layers):\n",
    "        super(SimpleTransformer, self).__init__() \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size) \n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_size, nhead=num_heads, num_encoder_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "     def forward(self, src, tgt):\n",
    "        src = self.embedding(src)\n",
    "        tgt = self.embedding(tgt)\n",
    "        return self.fc(self.transformer(src, tgt))\n",
    "# Example training data (encoded as numbers)\n",
    "src = torch.randint(0, vocab_size, (10, 32)) #10 sequences of length 32\n",
    "tgt = torch.randint(0, vocab_size, (10, 32))\n",
    "# Model, loss, and optimizer\n",
    "model = SimpleTransformer(vocab_size, embed_size, num_heads, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "for epoch in range(5): # Simplified training loop\n",
    "    optimizer.zero_grad()\n",
    "    output = model(src, tgt)\n",
    "    loss = criterion(output.view(-1, vocab_size), tgt.view(-1)) \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(\"Transformer model training complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple PyTorch model\n",
    "class SentimentNN(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(SentimentNN, self).__init__() \n",
    "        self.fc = nn.Linear(vocab_size, 1) \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return self.sigmoid(x)\n",
    "# Dummy training loop for epoch in range(10):\n",
    "for i, review in enumerate(encoded_reviews):\n",
    "    input_data = torch.tensor(review, dtype=torch.float32)  \n",
    "    label = torch.tensor([labels[i]], dtype=torch.float32)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_data)\n",
    "    loss = criterion(output, label) \n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "    print(\"Training complete!\")\n",
    "#Step 3: Inference - Using the Model for Predictions\n",
    "def predict_sentiment(review):\n",
    "    # Convert review into numerical format\n",
    "    review_words = review.split()\n",
    "    encoded_review = [vocab.get(word, 0) for word in review_words] # Default to 0 if word is not in vocab input_data = torch.tensor(encoded_review, dtype=torch.float32)\n",
    "    # Make prediction\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_data)\n",
    "        sentiment = \"Positive\" if prediction.item() >= 0.5 else \"Negative\"\n",
    "        return sentiment\n",
    "# Example usage\n",
    "new_review = \"This movie was amazing and thrilling!\"\n",
    "print(f\"Review: {new_review}\")\n",
    "print(f\"Predicted Sentiment: {predict_sentiment(new_review)}\")\n",
    "\n",
    "#Step 5: Building a Simplified Transformer Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "# Movie review vocabulary size (dummy data)\n",
    "vocab_size = 100\n",
    "embed_size = 64\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "class SimpleTransformer(nn.Module):\n",
    "     def __init__(self, vocab_size, embed_size, num_heads, num_layers):\n",
    "        super(SimpleTransformer, self).__init__() \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size) \n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_size, nhead=num_heads, num_encoder_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "     def forward(self, src, tgt):\n",
    "        src = self.embedding(src)\n",
    "        tgt = self.embedding(tgt)\n",
    "        return self.fc(self.transformer(src, tgt))\n",
    "# Example training data (encoded as numbers)\n",
    "src = torch.randint(0, vocab_size, (10, 32)) #10 sequences of length 32\n",
    "tgt = torch.randint(0, vocab_size, (10, 32))\n",
    "# Model, loss, and optimizer\n",
    "model = SimpleTransformer(vocab_size, embed_size, num_heads, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "for epoch in range(5): # Simplified training loop\n",
    "    optimizer.zero_grad()\n",
    "    output = model(src, tgt)\n",
    "    loss = criterion(output.view(-1, vocab_size), tgt.view(-1)) \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(\"Transformer model training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m input_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(review, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)  \n\u001b[1;32m      4\u001b[0m label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([labels[i]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m----> 5\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      6\u001b[0m output \u001b[38;5;241m=\u001b[39m model(input_data)\n\u001b[1;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, label) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Dummy training loop for epoch in range(10):\n",
    "for i, review in enumerate(encoded_reviews):\n",
    "    input_data = torch.tensor(review, dtype=torch.float32)  \n",
    "    label = torch.tensor([labels[i]], dtype=torch.float32)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_data)\n",
    "    loss = criterion(output, label) \n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "    print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
