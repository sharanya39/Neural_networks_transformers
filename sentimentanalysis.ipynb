{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-World Use Case: Movie Review Sentiment Analysis:\n",
    "\n",
    "    We'll simulate a scenario where our model predicts the sentiment of movie reviews. This approach is commonly used in the movie industry for analyzing audience feedback. Our final model will be able to infer whether a given movie review is positive or negative, helping movie studios gauge the reception of their films in real-time.\n",
    "\n",
    "    1.Understanding Neural Networks through Movie Reviews\n",
    "        We will develop a neural network to analyze movie reviews and predict if the sentiment is positive or negative. This model can be used by film studios or streaming platforms to automate sentiment analysis.\n",
    "    \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  2: Implementing Sentiment Analysis Using PyTorch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network training complete!\n",
      "Review: Hate this movie!\n",
      "Predicted Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Sample movie review data (simplified example)\n",
    "reviews = [\"I loved this movie!\", \"It was terrible.\", \"Absolutely fantastic!\", \"Not worth watching.\"]\n",
    "labels = [1, 0, 1, 0]  # 1 = positive, 0 = negative\n",
    "\n",
    "# Convert reviews into numerical data (simple encoding)\n",
    "vocab = {word: i for i, word in enumerate(set(\" \".join(reviews).split()))}\n",
    "vocab_size = len(vocab)\n",
    "encoded_reviews = [[vocab[word] for word in review.split()] for review in reviews]\n",
    "\n",
    "# Pad sequences to the same length\n",
    "max_len = max(len(review) for review in encoded_reviews)\n",
    "padded_reviews = [review + [0] * (max_len - len(review)) for review in encoded_reviews]\n",
    "\n",
    "# Define the Neural Network Model\n",
    "class SentimentNN(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(SentimentNN, self).__init__()\n",
    "        self.fc = nn.Linear(vocab_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = SentimentNN(vocab_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    for i, review in enumerate(padded_reviews):\n",
    "        input_data = torch.zeros(vocab_size)\n",
    "        input_data[review] = 1  # One-hot encoding\n",
    "        label = torch.tensor([labels[i]], dtype=torch.float32)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_data)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(\"Neural network training complete!\")\n",
    "\n",
    "# Inference Function\n",
    "def predict_sentiment(review):\n",
    "    review_words = review.split()\n",
    "    encoded_review = [vocab.get(word, 0) for word in review_words]\n",
    "    input_data = torch.zeros(vocab_size)\n",
    "    input_data[encoded_review] = 1  # One-hot encoding\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_data)\n",
    "        sentiment = \"Positive\" if prediction.item() >= 0.5 else \"Negative\"\n",
    "    return sentiment\n",
    "\n",
    "# Example usage\n",
    "new_review = \"Hate this movie!\"\n",
    "print(f\"Review: {new_review}\")\n",
    "print(f\"Predicted Sentiment: {predict_sentiment(new_review)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer model training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Model hyperparameters\n",
    "vocab_size = 100\n",
    "embed_size = 64\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "seq_length = 32\n",
    "\n",
    "# Define the Transformer Model\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, num_layers):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_size, \n",
    "            nhead=num_heads, \n",
    "            num_encoder_layers=num_layers, \n",
    "            num_decoder_layers=num_layers, \n",
    "            batch_first=True  # Enable batch_first for better performance\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # Embedding the inputs\n",
    "        src = self.embedding(src)\n",
    "        tgt = self.embedding(tgt)\n",
    "        # Pass through Transformer\n",
    "        output = self.transformer(src, tgt)\n",
    "        # Pass through final linear layer\n",
    "        return self.fc(output)\n",
    "\n",
    "# Example training data\n",
    "src = torch.randint(0, vocab_size, (10, seq_length))  # Batch of 10 sequences of length 32\n",
    "tgt = torch.randint(0, vocab_size, (10, seq_length))\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = SimpleTransformer(vocab_size, embed_size, num_heads, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the Transformer Model\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(src, tgt)\n",
    "    loss = criterion(output.view(-1, vocab_size), tgt.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Transformer model training complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
